{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import logging\n",
    "import tempfile\n",
    "import time\n",
    "from datetime import  datetime\n",
    "import dateparser\n",
    "import hashlib\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams, LTRect, LTTextBox, LTTextBoxHorizontal, \\\n",
    "        LTTextLine, LTTextLineHorizontal, LTChar\n",
    "\n",
    "def timeit(method):\n",
    "    \"\"\" Get method execution time \"\"\"\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "          name = kw.get('log_name', method.__name__.upper())\n",
    "          kw['log_time'][name] = int((te - ts))\n",
    "        else:\n",
    "            print('%r - %d heures %d minutes %2.2f secondes.'\\\n",
    "                  % (method.__name__, (te - ts) / 3600, (te - ts)%3600 / 60, (te - ts) % 60))\n",
    "        return result\n",
    "    return timed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrlsManager:\n",
    "    def __init__(self):\n",
    "        self.config = configparser.ConfigParser()\n",
    "        self.config.read('encans.ini')\n",
    "        self.deals_home_urls = self.config.get('site', 'deals_home_urls')\n",
    "        self.deals_histored_urls = self.config.get('site', 'deals_histored_urls')\n",
    "        self.deals_caming_urls = self.config.get('site', 'deals_caming_urls')\n",
    "        self.logger = logging.getLogger('encans')\n",
    "        \n",
    "        logfile = self.config.get('dir', 'datadir') + 'logs\\encans_urls_' +\\\n",
    "        str(datetime.now().date()) + '.log'\n",
    "        hdlr = logging.FileHandler(logfile)\n",
    "        formatter = logging.Formatter('%(asctime)s -- %(funcName)s -- %(levelname)s '\\\n",
    "                                      '-- %(message)s')\n",
    "        hdlr.setFormatter(formatter)\n",
    "        self.logger.addHandler(hdlr) \n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        self.logger.info('*********     *********     *********     *********     *********')\n",
    "        self.logger.info('*********     UrlsManager Started     *********')\n",
    "        \n",
    "      \n",
    "    def processUrls(self):\n",
    "        #Extract all histored urls\n",
    "        hUrls = self.extactHistoredUrls(self.deals_home_urls, self.deals_histored_urls)\n",
    "        col_names = [\"id\", \"city\", \"address\", \"url\", \"date\"]\n",
    "        hdf = pd.DataFrame(hUrls, columns=col_names)\n",
    "        hdf[\"status\"] = \"waiting\"\n",
    "        hdf[\"scraping_date\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        #Extract all news urls and merge them with histored urls\n",
    "        nUrls = self.extactNewUrls(self.deals_home_urls, self.deals_caming_urls)\n",
    "        ndf = pd.DataFrame(nUrls, columns=col_names)\n",
    "        ndf[\"status\"] = \"new\"\n",
    "        ndf[\"scraping_date\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        urls_df = hdf.append(ndf, sort=True)\n",
    "        \n",
    "        urls_repo_path = self.config.get('dir', 'datadir') + \\\n",
    "        'urls_to_scrape\\encan_urls_repo.csv'\n",
    "        \n",
    "        #Merge all extracted urls with the repo\n",
    "        old_urls_df = pd.read_csv(urls_repo_path, sep=self.config.get('conf', 'default_sep'), \\\n",
    "                              delimiter=None, header='infer')\n",
    "        self.logger.info('%d urls in the old Dataframe Repo' %len(old_urls_df.index))\n",
    "        new_urls_df = urls_df[~urls_df.id.isin(old_urls_df.id)]\n",
    "        self.logger.info('%d news urls to add to the Dataframe Repo' %len(new_urls_df.index))\n",
    "        urls_df = old_urls_df.append(new_urls_df, sort=True)\n",
    "        \n",
    "        urls_df.to_csv(urls_repo_path, sep=self.config.get('conf', 'default_sep'),\\\n",
    "                       index = False, encoding='utf-8')\n",
    "        self.logger.info('%d urls now in the Dataframe Repo' %len(urls_df.index))\n",
    "        print(\"Processing urls done !\")\n",
    "        return urls_df\n",
    "    \n",
    "    def extactHistoredUrls(self, homeUrl, historedUrl):\n",
    "        \"\"\"Extract histored deals urls\"\"\"\n",
    "        self.logger.info('Extracting deals histred urls form ' + historedUrl)\n",
    "        page = requests.get(historedUrl)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        urls = []\n",
    "        htmlList = soup.find(\"table\", \"lots\").find_all(\"tr\")\n",
    "        \n",
    "        for p in htmlList:\n",
    "            if p and p.th.get_text('strip=True') != 'Lieu':\n",
    "                city_adr = p.th.get_text('strip=True')\n",
    "                city = city_adr.split(\"-\")[0].strip()\n",
    "                address = \"\" if len(city_adr.split(\"-\")) == 1 else \\\n",
    "                city_adr.split(\"-\")[1].strip()          \n",
    "                date_txt = p.td.get_text('strip=True')[:-6].strip()\n",
    "                date = dateparser.parse(date_txt)\n",
    "\n",
    "                url = homeUrl + p.select(\"td a\")[0][\"href\"]\n",
    "                id = hashlib.sha1(url.encode('utf-8')).hexdigest()\n",
    "                urls.append([id, city, address, url, date])\n",
    "                \n",
    "        self.logger.info(\"%d histored urls extracted.\" %len(urls) )\n",
    "          \n",
    "        return urls\n",
    "    \n",
    "    def extactNewUrls(self, homeUrl, comingUrls):\n",
    "        \"\"\"Extract coming urls deals urls\"\"\"\n",
    "        self.logger.info('Extract coming urls deals urls ' + comingUrls)\n",
    "        page = requests.get(comingUrls)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        newUrls = []\n",
    "        htmlTRList = soup.find(\"table\", \"lots\").find_all(\"tr\")\n",
    "        \n",
    "        for tr in htmlTRList:\n",
    "            if tr:\n",
    "                if tr.th:\n",
    "                    th = tr.th\n",
    "                    if th.has_attr('class'):\n",
    "                        city_adr = th.a.get_text('strip=True')\n",
    "                        city = city_adr.split(\"-\")[0].strip()\n",
    "                        address = \"\" if len(city_adr.split(\"-\")) == 1 else\\\n",
    "                        city_adr.split(\"-\")[1].strip()     \n",
    "                        date_td = th.find_next_sibling(\"td\")\n",
    "                        date_txt = date_td.get_text('strip=True')[:-6].strip()\n",
    "                        date = dateparser.parse(date_txt)\n",
    "                        nextTDs = date_td.find_next_siblings(\"td\")\n",
    "                        for td in nextTDs:\n",
    "                            if td.a:\n",
    "                                if \"Catalogue_internet\" in td.select(\"a\")[0][\"href\"]:\n",
    "                                    url = homeUrl + td.select(\"a\")[0][\"href\"]\n",
    "                                    id = hashlib.sha1(url.encode('utf-8')).hexdigest()\n",
    "                                    newUrls.append([id, city, address, url, date])\n",
    "          \n",
    "        self.logger.info(\"%d coming urls extracted.\" %len(newUrls) )\n",
    "        return newUrls\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data from pdfs to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfExtractor:\n",
    "    def __init__(self):\n",
    "        self.config = configparser.ConfigParser()\n",
    "        self.config.read('encans.ini')\n",
    "        self.deals_home_urls = self.config.get('site', 'deals_home_urls')\n",
    "        self.deals_histored_urls = self.config.get('site', 'deals_histored_urls')\n",
    "        self.deals_caming_urls = self.config.get('site', 'deals_caming_urls')\n",
    "        self.logger = logging.getLogger('encans')\n",
    "        \n",
    "        logfile = self.config.get('dir', 'datadir') + 'logs\\encans_pdf_'\\\n",
    "        + str(datetime.now().date()) + '.log'\n",
    "        hdlr = logging.FileHandler(logfile)\n",
    "        formatter = logging.Formatter('%(asctime)s -- %(funcName)s -- %(levelname)s '\\\n",
    "                                      '-- %(message)s')\n",
    "        hdlr.setFormatter(formatter)\n",
    "        self.logger.addHandler(hdlr) \n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        self.logger.info('*********     *********     *********     *********     *********')\n",
    "        self.logger.info('*********     PdfExtractor Started     *********')\n",
    "    \n",
    "    \n",
    "    def flatten(self, lst):\n",
    "        \"\"\"Flattens a list of lists\"\"\"\n",
    "        return [subelem for elem in lst for subelem in elem]\n",
    "\n",
    "\n",
    "    def extract_characters(self, element):\n",
    "        \"\"\"\n",
    "        Recursively extracts individual characters from \n",
    "        text elements. \n",
    "        \"\"\"\n",
    "        TEXT_ELEMENTS = [LTTextBox, LTTextBoxHorizontal, LTTextLine, LTTextLineHorizontal]\n",
    "\n",
    "        if isinstance(element, LTChar):\n",
    "            return [element]\n",
    "\n",
    "        if any(isinstance(element, i) for i in TEXT_ELEMENTS):\n",
    "            return self.flatten([self.extract_characters(e) for e in element])\n",
    "\n",
    "        if isinstance(element, list):\n",
    "            return self.flatten([self.extract_characters(l) for l in element])\n",
    "\n",
    "        return []\n",
    "    \n",
    "    @timeit\n",
    "    def extract_layout_by_page(self, pdf_url):\n",
    "        \"\"\"\n",
    "        Extracts LTPage objects from a pdf file.\n",
    "        Slightly modified from\n",
    "        https://euske.github.io/pdfminer/programming.html\n",
    "        \"\"\"\n",
    "        laparams = LAParams()\n",
    "        req =  requests.get(pdf_url, stream = True) #Request the pdf content        \n",
    "        temp = tempfile.TemporaryFile() #Create a temporary file\n",
    "        temp.write(req.raw.data)\n",
    "        temp.seek(0)\n",
    "        parser = PDFParser(temp)\n",
    "        document = PDFDocument(parser)\n",
    "\n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        layouts = []\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            interpreter.process_page(page)\n",
    "            layouts.append(device.get_result())\n",
    "            \n",
    "        # Close object\n",
    "        temp.close()\n",
    "        device.close()\n",
    "\n",
    "        return layouts\n",
    "    \n",
    "    \n",
    "    def  getPageContent(self, page_number, page, pdf_id):\n",
    "        \"\"\"\n",
    "            Extact a pdf page to a DataFrame\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Processing pdf %s - page %d .\" %(pdf_id, page_number))\n",
    "        texts = []\n",
    "        rects = []\n",
    "\n",
    "        #Get page's text and rectangles. Seperate text and rectangle elements in 2 lists\n",
    "        for e in page:\n",
    "            if isinstance(e, LTTextBoxHorizontal):\n",
    "                texts.append(e)\n",
    "           \n",
    "            if isinstance(e, LTRect):\n",
    "                rects.append([e.x0, e.y0, e.x1, e.y1])\n",
    "                \n",
    "        recsDF =  pd.DataFrame(rects, columns=[\"x0\", \"y0\", \"x1\", \"y1\"])\n",
    "        recsDF[\"page_number\"] = page_number\n",
    "        recsDF[\"pdf_id\"] = pdf_id\n",
    "        \n",
    "        characters = self.extract_characters(texts)\n",
    "\n",
    "        charactersList = list(map(lambda x : [x.x0, x.y0, x.x1, x.y1, x.get_text(),\\\n",
    "                                              x.fontname, \\\n",
    "                                              x.adv, x.matrix], characters))\n",
    "        \n",
    "        charactersDF =  pd.DataFrame(charactersList, \\\n",
    "                                       columns=[\"x0\", \"y0\", \"x1\", \"y1\", \"text\",\\\n",
    "                                                \"fontname\", \"adv\", \"matrix\"])\n",
    "        charactersDF[\"page_number\"] = page_number\n",
    "        charactersDF[\"pdf_id\"] = pdf_id\n",
    "        \n",
    "        return [charactersDF, recsDF]\n",
    "\n",
    "    \n",
    "    @timeit\n",
    "    def  downloadPDFData(self, url_id, pdf_url):\n",
    "        \"\"\" Download a single pdf and store the result to csv files \"\"\"\n",
    "        self.logger.info(\"Downloading %s\" %pdf_url)\n",
    "\n",
    "        page_layouts = self.extract_layout_by_page(pdf_url)\n",
    "        self.logger.info('%s pages found in this pdf file' %str(len(page_layouts)))\n",
    "        \n",
    "        textDF = pd.DataFrame(columns=[\"x0\", \"y0\", \"x1\", \"y1\", \"text\", \"fontname\",\\\n",
    "                                       \"adv\", \"matrix\"])\n",
    "        rectsDF = pd.DataFrame(columns=[\"x0\", \"y0\", \"x1\", \"y1\"])\n",
    "        for page_num, current_page in enumerate(page_layouts, 1):\n",
    "            page_data = self.getPageContent(page_num, current_page, url_id)\n",
    "            textDF = textDF.append(page_data[0], sort=True)\n",
    "            rectsDF = rectsDF.append(page_data[1], sort=True)\n",
    "            \n",
    "        textFilesDir = self.config.get('dir', 'datadir') + \\\n",
    "            'scraping\\encans_text__' + url_id + '__' + str(datetime.now().date()) + '.csv'\n",
    "        rectsFilesDir = self.config.get('dir', 'datadir') + \\\n",
    "            'scraping\\encans_rects__' + url_id + '__' + str(datetime.now().date()) + '.csv'\n",
    "        \n",
    "        textDF.to_csv(textFilesDir, sep = self.config.get('conf', 'default_sep'), \\\n",
    "                      index = False, encoding='utf-8')\n",
    "        rectsDF.to_csv(rectsFilesDir, sep = self.config.get('conf', 'default_sep'), \\\n",
    "                       index = False, encoding='utf-8')\n",
    "        \n",
    "        return True\n",
    "\n",
    "      \n",
    "    @timeit\n",
    "    def  downloadDeals(self):\n",
    "        print('Downloading pdf files ...')\n",
    "        \"\"\" List all pdf to download and orchestrate the downloading processs \"\"\"\n",
    "        urls_file = self.config.get('dir', 'datadir')\\\n",
    "            +'/urls_to_scrape/encan_urls_repo.csv'\n",
    "\n",
    "        urls_repo_df = None\n",
    "        urls_to_scrape_df = None\n",
    "        \n",
    "        self.logger.info(\"Preparing to download pdf from file %s\" %urls_file)\n",
    "        \n",
    "        try:\n",
    "            urls_repo_df = pd.read_csv(urls_file, sep=self.config.get('conf', 'default_sep'), \\\n",
    "                              delimiter=None, header='infer')\n",
    "        except Exception  as e:\n",
    "            self.logger.error(\"ERROR when reading urls file\")\n",
    "            self.logger.error(\"Error: %s \" %str(e.args))\n",
    "            exc_type, exc_value, exc_traceback = sys.exc_info()          \n",
    "            self.logger.error(traceback.print_exception(exc_type, \\\n",
    "                                                        exc_value, exc_traceback, \\\n",
    "                                                        limit=2, file=sys.stdout))\n",
    "            return False\n",
    "            \n",
    "        urls_to_scrape_df = urls_repo_df[urls_repo_df['status'] != 'scraped']\n",
    "        self.logger.info(\"%d pdf to download.\" %len(urls_to_scrape_df.index))\n",
    "        \n",
    "        for index, row in urls_to_scrape_df.iterrows():\n",
    "            result_ = 'scraped' if self.downloadPDFData(row['id'],\\\n",
    "                                                        row['url']) else 'failed_download'\n",
    "            urls_repo_df.loc[urls_repo_df['id'] == row['id'], 'status'] = result_\n",
    "            urls_repo_df.loc[urls_repo_df['id'] == row['id'], 'scraping_date'] = \\\n",
    "                datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            urls_to_scrape_df.loc[urls_to_scrape_df['id'] == row['id'],\\\n",
    "                                  'status'] = result_\n",
    "            \n",
    "            \n",
    "            self.logger.info(\"   --------------------------------------------   \")\n",
    "            self.logger.info(\"File %d over %d\" %(index + 1, \\\n",
    "                                                 len(urls_to_scrape_df.index)))\n",
    "            self.logger.info(\"   --------------------------------------------   \")\n",
    "            print(\"File %d over %d\" %(index + 1, \\\n",
    "                                                 len(urls_to_scrape_df.index)))\n",
    "            \n",
    "        \n",
    "        urls_repo_df.to_csv(urls_file, sep=self.config.get('conf', 'default_sep'),\\\n",
    "                       index = False, encoding='utf-8')\n",
    "        \n",
    "        self.logger.info(\". . . . . . . . S T A T S . . . . . . . .\")\n",
    "        self.logger.info(str(urls_to_scrape_df.groupby('status').size()))\n",
    "        self.logger.info(\" . . . . END . . . . \")\n",
    "        print('Extracting pdf done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncansProcessor:\n",
    "    def __init__(self):\n",
    "        self.config = configparser.ConfigParser()\n",
    "        self.config.read('encans.ini')\n",
    "        self.deals_home_urls = self.config.get('site', 'deals_home_urls')\n",
    "        self.deals_histored_urls = self.config.get('site', 'deals_histored_urls')\n",
    "        self.deals_caming_urls = self.config.get('site', 'deals_caming_urls')\n",
    "        self.logger = logging.getLogger('encans')\n",
    "        \n",
    "        logfile = self.config.get('dir', 'datadir') + 'logs\\encans_proc__' \\\n",
    "        + str(datetime.now().date()) + '.log'\n",
    "        hdlr = logging.FileHandler(logfile)\n",
    "        formatter = logging.Formatter('%(asctime)s -- %(funcName)s -- '\\\n",
    "                                      '%(levelname)s -- %(message)s')\n",
    "        hdlr.setFormatter(formatter)\n",
    "        self.logger.addHandler(hdlr) \n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        self.logger.info('*********     *********     *********     *********     *********')\n",
    "        self.logger.info('*********     '+ str(self.__class__) +' Started     *********')\n",
    "    \n",
    "    @timeit\n",
    "    def processDeals(self):\n",
    "        print(\"Processing ...\")\n",
    "        scraping_dir = self.config.get('dir', 'datadir') + '/scraping/'\n",
    "        #scraping_dir = \"C:/Baawngal/Data Analysis/encans/Temps/\"\n",
    "        ### Read all .csv (rects and texts) to one pandans Dataframe\n",
    "        text_files_wildcard = glob.glob(scraping_dir + \"encans_text__*.csv\")\n",
    "        text_files_list = []\n",
    "        \n",
    "        rect_files_wildcard = glob.glob(scraping_dir + \"encans_rects__*.csv\")\n",
    "        rect_files_list = []\n",
    "\n",
    "        for file_ in text_files_wildcard:\n",
    "            t_df = pd.read_csv(file_,index_col=None, header=0, \\\n",
    "                             sep=self.config.get('conf', 'default_sep'))\n",
    "            text_files_list.append(t_df)\n",
    "            \n",
    "        self.logger.info(str(len(text_files_list)) + ' text files to process')\n",
    "            \n",
    "        for f in rect_files_wildcard:\n",
    "            r_df = pd.read_csv(f, index_col=None, header=0, \\\n",
    "                             sep=self.config.get('conf', 'default_sep'))\n",
    "            rect_files_list.append(r_df)\n",
    "            \n",
    "        self.logger.info(str(len(rect_files_list)) + ' rect files to process')\n",
    "            \n",
    "        \"\"\"\n",
    "        \"\"\"\"\"\" Processing Texts\n",
    "        \"\"\"\n",
    "        df_text = pd.concat(text_files_list, axis = 0, ignore_index = True)\n",
    "        \n",
    "        # Separate pdf right columns to left columns\n",
    "        df_text_agg = df_text.groupby(['pdf_id', 'page_number'], as_index=False)\\\n",
    "                             .agg({'x0':min, 'x1':max})\n",
    "        df_text_agg = df_text_agg.rename(columns={'x0':'page_min_x0', 'x1':'page_max_x1'})\n",
    "        df_text = pd.merge(df_text, df_text_agg, on=['pdf_id', 'page_number'], how='left')\n",
    "        df_text = df_text.assign(mid_x = lambda x: (x.page_min_x0 + x.page_max_x1)/2)\n",
    "        df_text[\"col_type\"] = np.where(df_text['x0']<= df_text['mid_x'], 'left', 'right')\n",
    "        \n",
    "        # Group text by lines        \n",
    "        df_lines = df_text.groupby(['pdf_id', 'page_number', 'y0', 'y1', 'col_type'], \\\n",
    "                                   as_index=False)\\\n",
    "                          .agg({'text': sum, 'x0':min, 'x1':max, 'fontname':max})\n",
    "        df_text = None\n",
    "        \n",
    "        \"\"\"\n",
    "        \"\"\"\"\"\" Processing Rects\n",
    "        \"\"\"\n",
    "        \n",
    "        df_rects = pd.concat(rect_files_list, axis = 0, ignore_index = True)\n",
    "        # Calculate width, hight, min - max - dix x and column type (roght or left)\n",
    "        df_rects = df_rects.assign(height = lambda x: x.y1 - x.y0) \\\n",
    "                           .assign(width = lambda x: x.x1 - x.x0) \n",
    "        df_rects_agg = df_rects.groupby(['pdf_id', 'page_number'], as_index=False)\\\n",
    "                               .agg({'x0':min, 'x1':max})\n",
    "        df_rects_agg = df_rects_agg.rename(columns={\"x0\":\"rect_page_min_x0\",\\\n",
    "                                                    \"x1\":\"rect_page_max_x1\"})\n",
    "        \n",
    "        df_rects = pd.merge(df_rects, df_rects_agg, on=['pdf_id', 'page_number'], how='left')\n",
    "        df_rects = df_rects.assign(rect_mid_x = lambda x: \\\n",
    "                                   (x.rect_page_min_x0 + x.rect_page_max_x1)/2)\n",
    "        df_rects[\"col_type\"] = \\\n",
    "            np.where(df_rects['x0']<= df_rects['rect_mid_x'], 'left', 'right')\n",
    "        \n",
    "        df_rects_agg = df_rects.groupby(['pdf_id','page_number','col_type'],as_index=False)\\\n",
    "                               .agg({'height':max, 'width':max})\n",
    "        df_rects_agg = df_rects_agg.rename(columns={\"height\":\"rect_page_max_height\", \\\n",
    "                                                    \"width\":\"rect_page_max_width\"})\n",
    "        df_rects = pd.merge(df_rects, df_rects_agg, on=['pdf_id','page_number','col_type'],\\\n",
    "                            how='left')\n",
    "        \"\"\"\n",
    "        \"\"\"\"\"\" Keep only interesting rects\n",
    "        \"\"\"\n",
    "        # Keep rects with width enought large\n",
    "        df_rects = df_rects[df_rects.width >= df_rects.rect_page_max_width * 0.7]\n",
    "        \n",
    "        # Sort rects and get the following rect.y0 for each rect, calculate the gap between\n",
    "        # this rect and it's following. Then get the max gap by page in order to eliminate\n",
    "        # non useful rects\n",
    "        df_rects = df_rects.sort_values(['pdf_id','page_number','col_type', 'y0'],\\\n",
    "                                            ascending=[True, True, True, False])\n",
    "        df_rects['rect_next_y0'] = df_rects['y0'].shift(-1)\n",
    "        df_rects['rect_gap_y0'] = df_rects['y0'] - df_rects['rect_next_y0']\n",
    "        \n",
    "        df_rects_agg = df_rects.groupby(['pdf_id','page_number','col_type'],as_index=False)\\\n",
    "                               .agg({'rect_gap_y0':max})\n",
    "        df_rects_agg = df_rects_agg.rename(columns={\"rect_gap_y0\":\"page_max_gap_y0\"})\n",
    "        df_rects = pd.merge(df_rects, df_rects_agg, on=['pdf_id','page_number','col_type'],\\\n",
    "                            how='left')\n",
    "        \n",
    "        # Keep rects with enought large gap (ignore rects wich are too close to\n",
    "        # their follower)\n",
    "        df_rects = df_rects[df_rects.rect_gap_y0 >= df_rects.page_max_gap_y0 * 0.7]\n",
    "\n",
    "        # Join texts and rects on pdf_id, page_number, col_type and text y0 between \n",
    "        # rect rect y0 and next rect y0\n",
    "        df_lines = pd.merge(df_lines, df_rects, on=['pdf_id', 'page_number', 'col_type'],\\\n",
    "                            how='left', suffixes=('', '_rect'))\n",
    "        \n",
    "        df_lines = df_lines[(df_lines.y0 <= df_lines.y0_rect) & \\\n",
    "                            (df_lines.y0 >= df_lines.rect_next_y0)] \n",
    "        \n",
    "        #Pivot data\n",
    "        df_lines['col_id'] =\\\n",
    "            df_lines.groupby(['pdf_id','page_number','col_type', 'y0_rect'])\\\n",
    "                    .cumcount(ascending=False)+1\n",
    "        \n",
    "        df_lines = df_lines.sort_values(['pdf_id','page_number','col_type', 'y0'],\\\n",
    "                                            ascending=[True, True, True, False])\n",
    "        df_lines = pd.pivot_table(df_lines,\\\n",
    "                                  index=['pdf_id','page_number','col_type', 'y0_rect'],\\\n",
    "                                  values=['text'], columns=['col_id'], aggfunc=np.max)\n",
    "        df_lines.reset_index()\n",
    "        df_lines = pd.DataFrame(df_lines.to_records())\n",
    "        df_lines.columns = [name.replace(\"('text', \", \"col\").replace(\")\", \"\")\\\n",
    "                            for name in df_lines.columns]\n",
    "        \n",
    "        self.logger.info('Dataframe generated =======>> ')\n",
    "        \n",
    "        self.logger.info(str(df_lines.info()))\n",
    "        \n",
    "        dataFile = self.config.get('dir', 'datadir') + \\\n",
    "            'scraping\\encans_data_1__' + str(datetime.now().date()) + '.csv'\n",
    "        df_lines.to_csv(dataFile, sep = self.config.get('conf', 'default_sep'), \\\n",
    "                      index = False, encoding='utf-8')\n",
    "        self.logger.info('Data saved to ' + dataFile)\n",
    "        \n",
    "        print('Processing done!')\n",
    "\n",
    "        return df_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing urls done !\n",
      "Downloading pdf files ...\n",
      "'extract_layout_by_page' - 0 heures 0 minutes 8.03 secondes.\n",
      "'downloadPDFData' - 0 heures 0 minutes 9.31 secondes.\n",
      "File 68 over 1\n",
      "Extracting pdf done\n",
      "'downloadDeals' - 0 heures 0 minutes 9.38 secondes.\n",
      "Processing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:178: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1456 entries, 0 to 1455\n",
      "Data columns (total 15 columns):\n",
      "pdf_id         1456 non-null object\n",
      "page_number    1456 non-null float64\n",
      "col_type       1456 non-null object\n",
      "y0_rect        1456 non-null float64\n",
      "col1           1456 non-null object\n",
      "col2           1456 non-null object\n",
      "col3           1456 non-null object\n",
      "col4           1455 non-null object\n",
      "col5           1445 non-null object\n",
      "col6           1434 non-null object\n",
      "col7           1403 non-null object\n",
      "col8           1363 non-null object\n",
      "col9           1200 non-null object\n",
      "col10          663 non-null object\n",
      "col11          77 non-null object\n",
      "dtypes: float64(2), object(13)\n",
      "memory usage: 170.7+ KB\n",
      "Processing done!\n",
      "'processDeals' - 0 heures 0 minutes 5.41 secondes.\n"
     ]
    }
   ],
   "source": [
    "#Process urls\n",
    "um = UrlsManager()\n",
    "df = um.processUrls()\n",
    "\n",
    "pe = PdfExtractor()\n",
    "pe.downloadDeals()\n",
    "ep = EncansProcessor()\n",
    "df = ep.processDeals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
